**Wikipedia Scraper (ETL Pipeline)
Overview**

This project implements a Python-based ETL (Extract, Transform, Load) pipeline to collect and process structured metadata from Wikipedia pages for analytical use cases.

ETL Workflow

Extract: Scrapes HTML content from Wikipedia using BeautifulSoup

Transform: Cleans, normalizes, and structures raw data into schema-ready formats

Load: Outputs transformed data for ingestion into relational databases or analytics platforms

Tech Stack

Python

BeautifulSoup

pandas

SQL / Database-ready outputs

Use Cases

Data analysis and reporting

Market and content research

Downstream BI and analytics workflows
